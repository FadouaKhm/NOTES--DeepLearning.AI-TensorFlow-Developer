{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing in TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FadouaKhm/NOTES--DeepLearning.AI-TensorFlow-Developer/blob/master/Natural_Language_Processing_in_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%201%20-%20Lesson%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "BZSlp3DAjdYf",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh4saWGYW7mt",
        "colab_type": "text"
      },
      "source": [
        "#  Week 1 - Lesson 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS7HhNSRX0vR",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow and keras give us a number of ways to encode words, but the one I'm going to focus on is the tokenizer. This will handle the heavy lifting for us, generating the dictionary of word encodings and creating vectors out of the sentences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zaCMcjMQifQc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "113fb1a5-0b4f-4560-8c20-3c4aad660610"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100) #Provides a word index dict() with values = code and keys = word, It also handles ponctuation and upper/lower cases\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zkIpJKcYkdU",
        "colab_type": "text"
      },
      "source": [
        "**Encoding list of words**\n",
        "\n",
        "Issues:\n",
        "1. Handling unseen words\n",
        "2. Tackling sentences with different sizes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ow01wUgYPvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5f1cfdff-cb1a-4162-b843-bc2aa21821c7"
      },
      "source": [
        "#1. Handling unseen words\n",
        "# Define out of vocab token for unseen words when defining the Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!',\n",
        "    'Emily loves her dog'\n",
        "]\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<oov>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(word_index)\n",
        "print(sequences)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<oov>': 1, 'love': 2, 'my': 3, 'dog': 4, 'i': 5, 'cat': 6, 'you': 7, 'emily': 8, 'loves': 9, 'her': 10}\n",
            "[[5, 2, 3, 4], [5, 2, 3, 6], [7, 2, 3, 4], [8, 9, 10, 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob3D0g1Fa13_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "637a6c75-78a7-423a-bb58-a739f188ac27"
      },
      "source": [
        "#2. Tackling sentences with different sizes\n",
        "# Define Padding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!',\n",
        "    'Emily loves her dog Rex',\n",
        "    'My dinner is yummier than yours lol!'\n",
        "]\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<oov>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded = pad_sequences(sequences)\n",
        "print(\"Word encoding : \", word_index)\n",
        "print(\" Original encoded sequences : \", sequences)\n",
        "print(\" Padded sequences : \", padded)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word encoding :  {'<oov>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'cat': 6, 'you': 7, 'emily': 8, 'loves': 9, 'her': 10, 'rex': 11, 'dinner': 12, 'is': 13, 'yummier': 14, 'than': 15, 'yours': 16, 'lol': 17}\n",
            " Original encoded sequences :  [[5, 3, 2, 4], [5, 3, 2, 6], [7, 3, 2, 4], [8, 9, 10, 4, 11], [2, 12, 13, 14, 15, 16, 17]]\n",
            " Padded sequences :  [[ 0  0  0  5  3  2  4]\n",
            " [ 0  0  0  5  3  2  6]\n",
            " [ 0  0  0  7  3  2  4]\n",
            " [ 0  0  8  9 10  4 11]\n",
            " [ 2 12 13 14 15 16 17]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cp8CCr4cXxm",
        "colab_type": "text"
      },
      "source": [
        "**Parameters to play with**\n",
        "\n",
        "\n",
        "*   padding = 'post' (default is 'pre')\n",
        "*   maxlen = 5 (default is length of longest sentence)\n",
        "*   If maxlen is set, decide from where to truncate if a sentence is longer than maxlen. Set truncate = 'post' (or 'pre')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1Bk-JU2brVz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "51119dc7-8987-445d-d053-08bab93adf06"
      },
      "source": [
        "# Try with words that the tokenizer wasn't fit to\n",
        "test_data = [\n",
        "    'i really love my dog',\n",
        "    'my dog loves my manatee'\n",
        "]\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(\"\\nTest Sequence = \", test_seq)\n",
        "\n",
        "padded = pad_sequences(test_seq, maxlen=10)\n",
        "print(\"\\nPadded Test Sequence: \")\n",
        "print(padded)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test Sequence =  [[5, 1, 3, 2, 4], [2, 4, 9, 2, 1]]\n",
            "\n",
            "Padded Test Sequence: \n",
            "[[0 0 0 0 0 5 1 3 2 4]\n",
            " [0 0 0 0 0 2 4 9 2 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr5S3gjwdxY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}